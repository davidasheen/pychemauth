{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4110398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:57:21.723251Z",
     "start_time": "2023-09-11T18:57:21.715643Z"
    }
   },
   "source": [
    "Soft Independent Modeling of Class Analogies (SIMCA)\n",
    "===\n",
    "\n",
    "Author: Nathan A. Mahynski\n",
    "\n",
    "Date: 2023/09/12\n",
    "\n",
    "Description: Derivation and examples of [SIMCA](https://en.wikipedia.org/wiki/Soft_independent_modelling_of_class_analogies).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahynski/pychemauth/blob/main/docs/jupyter/gallery/simca.ipynb)\n",
    "\n",
    "Soft Independent Modelling of Class Analogies (SIMCA) is a popular method for building authentication models.  Like Soft PLS-DA, it can determine if a new sample is consistent with a training set of known authentic class examples.  However, unlike PLS-DA, SIMCA models are trained on a single class and return a binary True/False prediction as to whether a new sample is consistent with previously seen ones.  There have been many variations on the technique since its [introduction by Wold](https://www.sciencedirect.com/science/article/abs/pii/0031320376900145) in the late 1970s.  We will review 2 different approaches: one more conventional, another which is more modern.\n",
    "\n",
    "Furthermore, there are differences in the way each model may be optimized. \"Rigorous\" models use only examples of the target class and are designed to reach a certain sensitivity (specificity cannot be evaluated); \"compliant\" models are instead trained using alternative examples to reach an overall ideal balance of specificity and sensitivity.  While the latter often appear to perform \"better,\" the results are biased based on what alternatives are available for training which is difficult to fully quantify. See [Rodionova et al.](https://www.sciencedirect.com/science/article/pii/S0169743916302799) for more discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b5c9e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T18:06:10.269967Z",
     "start_time": "2023-09-20T18:06:10.263293Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9) # Automatically restart the runtime to reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f60adae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T18:06:10.875354Z",
     "start_time": "2023-09-20T18:06:10.432207Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pychemauth\n",
    "except:\n",
    "    raise ImportError(\"pychemauth not installed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a350867e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:51:20.091225Z",
     "start_time": "2023-09-20T19:51:19.998586Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pychemauth.classifier.simca import SIMCA_Model\n",
    "from pychemauth.preprocessing.scaling import CorrectedScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03266b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T18:06:11.789370Z",
     "start_time": "2023-09-20T18:06:11.769520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "Compiler    : GCC 12.2.0\n",
      "OS          : Linux\n",
      "Release     : 6.2.0-26-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 40\n",
      "Architecture: 64bit\n",
      "\n",
      "matplotlib: 3.7.2\n",
      "json      : 2.0.9\n",
      "pychemauth: 0.0.0b3\n",
      "watermark : 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dd3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T18:46:38.499511Z",
     "start_time": "2023-09-20T18:46:38.475098Z"
    }
   },
   "source": [
    "A Conventional Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc0fa0",
   "metadata": {},
   "source": [
    "See [\"Robust classification in high dimensions based on the SIMCA Method,\" Vanden Branden, Hubert, Chemometrics and Intelligent Laboratory Systems 79 (2005) 10-21.](https://doi.org/10.1016/j.chemolab.2005.03.002) and [\"Decision criteria for soft independent modelling of class analogy applied to near infrared data\" De Maesschalk et al., Chemometrics and Intelligent Laboratory Systems 47 (1999) 65-77.](https://doi.org/10.1016/S0169-7439(98)00159-2) for details and justification of this implementation.\n",
    "\n",
    "Step 1: The raw data is **broken up by group (supervised)**; then **for each group** a PCA model for the data is constructed as follows:\n",
    "    \n",
    "$$\n",
    "X = TP^T + E.\n",
    "$$\n",
    "\n",
    "Here, $X$, is the training data and has dimensions IxJ where $T$ is the scores matrix (projection of $X$ into some space, IxK, determined by), $P$ is related to the loading matrix (JxK), and $E$ as the error or residual matrix.  $E$ may be explicitly calculated by $E = X - TP^T$. $X$ should be centered, and possibly scaled, as is required for PCA.\n",
    "\n",
    "<!-- As in PLS-DA, or other discriminant methods, a hard boundary between classes can be defined by drawing a hyperplane that divides them.  This leads to a hard \"yes\" or \"no\" if a point belongs to a certain class, and it can only belong to a single class.  Methods like SIMCA can predict if a point belongs to class A, class B, both, or neither.  It does this by defining a model boundary (distance from class centroid, for example) which envelops samples from class A; boundaries around different classes can be disjoint or overlapping, and their union does not need to entirely fill space (so that some points could belong to no classes).  Note the similarity to [Soft PLS-DA](plsda.ipynb); however, PLS-DA is trained with a fixed number of classes, whereas each SIMCA model is trained on only a single one. -->\n",
    "\n",
    "Step 2: The residual standard deviation (RSD) values are calculated for the test set and training set a little differently.  For a given observation in the test set:\n",
    "\n",
    "$$\n",
    "RSD_{i, test}^2 = \\frac{e_i^Te_i}{J-K},\n",
    "$$\n",
    "\n",
    "while for the training set (composed of the I samples) the degrees of freedom are modified slightly:\n",
    "\n",
    "$$\n",
    "RSD_{train}^2 = \\frac{\\sum_{i,train} e_i^Te_i}{(J-K)(I-K-1)}.\n",
    "$$\n",
    "\n",
    "Here, $e_i^Te_i$ is referred to as the squared orthogonal distance, $OD^2$. \n",
    "\n",
    "Step 3: The threshold for the model is given by:\n",
    "\n",
    "$$\n",
    "F_i = \\frac{RSD_{i, test}^2}{RSD_{\\rm train}^2}\n",
    "$$\n",
    "\n",
    "where the $F$ value is compared to some critical limiting value taken at a given significance, for example, $F_{\\rm crit} = F_{(J-K),(I-K-1);0.95}$ is commonly used as a 95% quantile limit.  If $F_i/F_{\\rm crit} < 1$, we assign the observation to the class.\n",
    "\n",
    "Note that the number of principal components, K, does NOT need to the same from group to group; in fact, this is where the \"independent\" part of S(I)MCA comes from. (see Vanden Branden et al.)\n",
    "\n",
    "Note: there is apparently some discrepancy, historically, on what degrees of freedom to use when computing these F statistics.  In De Maesschalk et al. Chem. Intell. Lab. Sys. 47 (1999) the authors discuss this in detail; essentially, in the above equations the term (J - K) is only valid when I > J (more samples than variables).  Otherwise, then term should be replaced with ((I - 1) - K) in both the test and train cases (and for computing the critical F value); i.e., use the smaller of (I,J), but due to mean centering we lose 1 DoF from I.  However, this term cancels out when computing the f value for a given sample.  It does, however, **affect the calculation of the critical F value**.  From a statistics perspective this is important so that $\\alpha$ is meaningful; from a machine learning perspective, if we simply chose to optimize $\\alpha$ to balance sensitivity and specificity of a model, then it is effectively irrelevant since this procedure treats $F_{\\rm crit}$ as an adjustable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e922b8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:51:22.283116Z",
     "start_time": "2023-09-20T19:51:22.257293Z"
    }
   },
   "outputs": [],
   "source": [
    "class SIMCA:\n",
    "    def __init__(self, n_components, alpha=0.05):\n",
    "        self.__n_components = n_components\n",
    "        self.__alpha = alpha\n",
    "\n",
    "    def fit(self, X_train):\n",
    "        # 1. Autoscale X\n",
    "        self.__ss = CorrectedScaler(with_mean=True, with_std=True)\n",
    "        self.__X_train = X_train.copy()\n",
    "\n",
    "        # 2. Perform PCA on standardized coordinates\n",
    "        self.__pca = PCA(n_components=self.__n_components, random_state=0)\n",
    "        self.__pca.fit(self.__ss.fit_transform(self.__X_train))\n",
    "\n",
    "        # 3. Compute critical F value\n",
    "        self.__I = X_train.shape[0]\n",
    "        self.__J = X_train.shape[1]\n",
    "        self.__K = self.__n_components\n",
    "        self.__a = self.__J if self.__I > self.__J else self.__I-1\n",
    "        \n",
    "        self.__f_crit = scipy.stats.f.ppf(\n",
    "            1.0-self.__alpha,\n",
    "            (self.__a-self.__K),\n",
    "            (self.__a-self.__K)*(self.__I-self.__K-1)\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check that observations are rows of X\n",
    "        X = np.array(X)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        assert(X.shape[1] == self.__J)\n",
    "\n",
    "        \"\"\"\n",
    "        From \"Multivariate class modeling for the verification of food-authenticity\n",
    "        claims,\" Oliveri and DOwney, TrAC 35 (2012):\n",
    "        \n",
    "        \"The distances between each sample and the model are\n",
    "        evaluated in the full multidimensional space, which is\n",
    "        also defined by the non-significant PCs (SIMCA outer\n",
    "        space). This permits the inclusion of information on\n",
    "        random distribution of samples around the model, which\n",
    "        arises from non-significant, and therefore uninformative,\n",
    "        variations.\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Following Vanden Branden et al.\n",
    "        X_pred = self.__ss.inverse_transform(\n",
    "            self.__pca.inverse_transform(\n",
    "                self.__pca.transform(\n",
    "                    self.__ss.transform(X)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        numer = np.sum((X - X_pred)**2, axis=1)/(self.__a - self.__K)\n",
    "\n",
    "        X_pred = self.__ss.inverse_transform(\n",
    "            self.__pca.inverse_transform(\n",
    "                self.__pca.transform(\n",
    "                    self.__ss.transform(self.__X_train)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        OD2 = np.sum((self.__X_train - X_pred)**2, axis=1)\n",
    "        denom = np.sum(OD2) / ((self.__a - self.__K)*(self.__I - self.__K - 1))\n",
    "\n",
    "        # F-test for each distance\n",
    "        F = numer/denom\n",
    "\n",
    "        # If f < f_crit, it belongs to the class\n",
    "        return F < self.__f_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b64673fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:51:22.666655Z",
     "start_time": "2023-09-20T19:51:22.635084Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_blobs(\n",
    "    n_samples=100,\n",
    "    n_features=5,\n",
    "    centers=3,\n",
    "    cluster_std=3,\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "X_train, y_train = X[:80], Y[:80]\n",
    "X_test, y_test = X[80:], Y[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f4cfb5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:55:21.326184Z",
     "start_time": "2023-09-20T19:55:21.276654Z"
    }
   },
   "outputs": [],
   "source": [
    "# WE need a new SIMCA object for EACH training class - each one indicates whether we predict a point\n",
    "# belongs to that class or not.\n",
    "manual_simca_model = {}\n",
    "for i in range(3):\n",
    "    manual_simca_model[i] = SIMCA(n_components=1, alpha=0.05)\n",
    "    manual_simca_model[i].fit(X_train[y_train == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0f706ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:55:25.961265Z",
     "start_time": "2023-09-20T19:55:25.916020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_simca_model[0].predict(X_test) == (y_test == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "821bdfba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:55:47.329174Z",
     "start_time": "2023-09-20T19:55:47.279320Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyChemAuth provides a more versatile class for this.\n",
    "pychemauth_simca_model = {}\n",
    "for i in range(3):\n",
    "    pychemauth_simca_model[i] = SIMCA_Model(n_components=1, alpha=0.05, scale_x=True)\n",
    "    pychemauth_simca_model[i].fit(X_train[y_train == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e37b819a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:55:51.102254Z",
     "start_time": "2023-09-20T19:55:51.056143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychemauth_simca_model[0].predict(X_test) == (y_test == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37e330f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:56:39.145176Z",
     "start_time": "2023-09-20T19:56:39.095336Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in [0, 1, 2]:\n",
    "    assert np.all(pychemauth_simca_model[i].predict(X_test) == manual_simca_model[i].predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094cc836",
   "metadata": {},
   "source": [
    "A Modern Approach: DD-SIMCA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad8f66",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/nathan-mahynski/nathan-mahynski.github.io/blob/public/_examples/common_chemometrics/example.ipynb#scrollTo=pODi0zhQ-ISp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1d4b0",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/12aajEL8tzkKEiGoI54xpH0-KlMa8U6V3#scrollTo=BqgfgRpE14_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Also check scaling in h and q space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oyster-provenance",
   "language": "python",
   "name": "oyster-provenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
