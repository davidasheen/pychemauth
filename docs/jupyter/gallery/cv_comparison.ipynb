{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ee5ed3",
   "metadata": {},
   "source": [
    "Statistical Comparison between Models using Cross-Validation\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569652ad",
   "metadata": {},
   "source": [
    "Author: Nathan A. Mahynski\n",
    "\n",
    "Date: 2023/09/07\n",
    "\n",
    "Description: Using cross validation to interrogate differences in model performance.\n",
    "\n",
    "In the previous notebook models were compared using nested cross-validation so that an uncertainty on the generalization error could be obtained.  **The question becomes, how do we know if these differences are statistically significant?** In principle, we can then perform standard hypothesis tests to check since we have means and uncertainties.  However, the main problem with this is that performance estimates are not independent of each other.  This tends to result in higher type I error (incorrectly reject the null hypothesis, which is two algorithms are the same) which means **you tend to see a difference in the performances when there isn't any.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d41e5",
   "metadata": {},
   "source": [
    "<!-- Another issue is that many of these approaches do not account for hyperparameter selection.  They tend to use disjoint training sets to train the model and evaluate on a separate test set, but if you have to do hyperameter selection usually the final model is selected based on which performs the best on these test sets.  As a result that test performance is biased - this is the reason a separate validation set is usually introduced.  The methods below neglect this, but [recent work](https://arxiv.org/pdf/1809.09446.pdf) suggests that, while this bias exists, it does not strongly affect the relative ranking of models or pipelines.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840d4a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:48.727392Z",
     "start_time": "2023-09-08T20:19:48.333338Z"
    }
   },
   "outputs": [],
   "source": [
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
    "\n",
    "try:\n",
    "    import pychemauth\n",
    "except:\n",
    "    raise ImportError(\"pychemauth not installed\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee1613e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:23:25.104975Z",
     "start_time": "2023-09-08T20:23:24.971391Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn import datasets\n",
    "\n",
    "from pychemauth.analysis.compare import BiasedNestedCV\n",
    "from pychemauth.analysis.compare import Compare\n",
    "from pychemauth.preprocessing.scaling import CorrectedScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf8caea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:49.238396Z",
     "start_time": "2023-09-08T20:19:49.214302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.4\n",
      "IPython version      : 8.14.0\n",
      "\n",
      "Compiler    : GCC 12.2.0\n",
      "OS          : Linux\n",
      "Release     : 6.2.0-26-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 40\n",
      "Architecture: 64bit\n",
      "\n",
      "scipy     : 1.11.1\n",
      "pychemauth: 0.0.0b3\n",
      "watermark : 2.4.3\n",
      "numpy     : 1.24.4\n",
      "matplotlib: 3.7.2\n",
      "sklearn   : 1.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370c2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T13:49:28.010723Z",
     "start_time": "2023-09-08T13:49:27.983017Z"
    }
   },
   "source": [
    "<h3>Load the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c41a8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:49.270199Z",
     "start_time": "2023-09-08T20:19:49.244287Z"
    }
   },
   "outputs": [],
   "source": [
    "data = datasets.load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829090ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:49.368293Z",
     "start_time": "2023-09-08T20:19:49.271772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's re-use these models from the last example.\n",
    "def make_svc(k_inner, random_state_inner=42):\n",
    "    pipe_svc = Pipeline(steps=[\n",
    "        (\"scaler\", CorrectedScaler()),\n",
    "        (\"model\", SVC())\n",
    "    ])\n",
    "    gs_svc = GridSearchCV(\n",
    "        estimator=pipe_svc,\n",
    "        param_grid=[{'model__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}],\n",
    "        scoring='accuracy',\n",
    "        cv=sklearn.model_selection.StratifiedKFold( # Since this is a classification problem we will stratify\n",
    "            n_splits=k_inner,\n",
    "            shuffle=True,\n",
    "            random_state=random_state_inner\n",
    "        ),\n",
    "        refit=True\n",
    "    )\n",
    "    \n",
    "    return gs_svc\n",
    "\n",
    "def make_tree(k_inner, random_state_inner=42):\n",
    "    gs_tree = GridSearchCV(\n",
    "        estimator=DecisionTreeClassifier(random_state=0),\n",
    "        param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n",
    "        scoring='accuracy',\n",
    "        cv=sklearn.model_selection.StratifiedKFold( # Since this is a classification problem we will stratify\n",
    "            n_splits=k_inner,\n",
    "            shuffle=True,\n",
    "            random_state=random_state_inner\n",
    "        ),\n",
    "        refit=True\n",
    "    )\n",
    "    \n",
    "    return gs_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33f489",
   "metadata": {},
   "source": [
    "The resampled paired t-test\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f56e35",
   "metadata": {},
   "source": [
    "The first way to obtain uncertainties on generalization performance is simply to shuffle the data and repeat the holdout procedure to get many different test set performances.  Statistical significance can be assessed by obtaining the mean and standard deviation for each model and performing a paired [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) on the differences.  If we use the same data split then both models are tested on the same \"chunk\" so we should use the paired version. However, a key assumption of the t-test is that measurements are independent; as you can see below for a small subset of data, reusing the data like this means points reoccur in many training (and test) sets, meaning each result is not independent.  This has been [shown](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf) to yield an unacceptably high [type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) (detection of a difference between algorithms when there is none).\n",
    "\n",
    "> Type I error is the rejection of the null hypothesis.  In [frequentist statistics](https://www.statisticshowto.com/frequentist-statistics/) the null hypothesis is usually formulated so that the onus is one proving a difference between to things (e.g., the difference of 2 means), and is usually [stated as](https://www.statisticshowto.com/probability-and-statistics/null-hypothesis/#state) the currently accepted [truth](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/).  The burden of proof then falls on the researcher to disprove this and reject the current status quo in favor of something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c273bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:50.326307Z",
     "start_time": "2023-09-08T20:19:50.289795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set on fold 1 :\n",
      " [[11.62  1.99]\n",
      " [13.69  3.26]\n",
      " [12.69  1.53]]\n",
      "Test set on fold 1 :\n",
      " [[13.4   3.91]\n",
      " [13.5   1.81]]\n",
      "---------------------------------\n",
      "Training set on fold 2 :\n",
      " [[11.62  1.99]\n",
      " [12.69  1.53]\n",
      " [13.5   1.81]]\n",
      "Test set on fold 2 :\n",
      " [[13.69  3.26]\n",
      " [13.4   3.91]]\n",
      "---------------------------------\n",
      "Training set on fold 3 :\n",
      " [[11.62  1.99]\n",
      " [13.5   1.81]\n",
      " [12.69  1.53]]\n",
      "Test set on fold 3 :\n",
      " [[13.4   3.91]\n",
      " [13.69  3.26]]\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for loop in range(3):\n",
    "    X_, y_ = sklearn.utils.shuffle(X_train[:5,:2], y_train[:5], random_state=loop)\n",
    "    X_train_ = X_[:3]\n",
    "    X_test_ = X_[3:]\n",
    "    print('Training set on fold {} :\\n {}'.format(loop+1, X_train_))\n",
    "    print('Test set on fold {} :\\n {}'.format(loop+1, X_test_))\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a29a18",
   "metadata": {},
   "source": [
    "The k-fold CV paired t-test\n",
    "---\n",
    "\n",
    "A slight improvement: split data into k disjoint folds (as in CV), and **using the same data split** evaluate each model.  The problem is now that each data point contributes to exactly $k-1$ training sets; therefore the scores estimated from the remaining fold will still be correlated, in general, because they are trained (and tested) on some of the same data.  However, note that when $k=2$ the data overlap problem goes away! The tradeoff is that we have only 2 estimates of the performance (2 test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e86e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:51.127906Z",
     "start_time": "2023-09-08T20:19:51.090202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  6  9 10 11 13 14 16 17 19] [ 0  2  3  4  5  7  8 12 15 18]\n",
      "[ 0  2  3  4  5  7  8 12 15 18] [ 1  6  9 10 11 13 14 16 17 19]\n"
     ]
    }
   ],
   "source": [
    "cv = sklearn.model_selection.StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in cv.split(X_train[:20], y_train[:20]):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c4dd49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:51.558967Z",
     "start_time": "2023-09-08T20:19:51.513688Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can then evalute a model on each test set using cross-validation\n",
    "scores = cross_val_score(\n",
    "    estimator=DecisionTreeClassifier(random_state=0, max_depth=3), \n",
    "    X=X_train[:20],\n",
    "    y=y_train[:20],\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfc1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T14:03:57.849142Z",
     "start_time": "2023-09-08T14:03:57.809594Z"
    }
   },
   "source": [
    "This produces k estimates which we can get a mean and uncertainty from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933eb0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:52.304029Z",
     "start_time": "2023-09-08T20:19:52.268935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw scores: [0.6 0.5]\n",
      "Mean : 0.550\n",
      "St. Dev. : 0.050\n"
     ]
    }
   ],
   "source": [
    "print('Raw scores: {}\\nMean : {}\\nSt. Dev. : {}'.format(scores, '%.3f'%np.mean(scores), '%.3f'%np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162b88c",
   "metadata": {},
   "source": [
    "The repeated k-fold CV paired t-test\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d563f",
   "metadata": {},
   "source": [
    "Using $k=2$ helped remove the some bias due to re-use of data in training sets, but 2 error estimates is really not enough to be very helpful.  [Dietterich popularized](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf) a 5x2 repeated CV scheme.  This essentially combines both ideas from the last section by shuffling the data differently 5 times, then doing 2-fold CV in each shuffle.  Of course, this re-uses data due to the reshuffling and so estimates are not completely independent, but was shown to have a tolerably elevated [Type I error rate](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors).  Still, this gives us 10 estimates instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5468f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:54.129834Z",
     "start_time": "2023-09-08T20:19:54.087240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[0 3 5 6 7]\n",
      "  Test:  index=[1 2 4 8 9]\n",
      "Fold 1:\n",
      "  Train: index=[1 2 4 8 9]\n",
      "  Test:  index=[0 3 5 6 7]\n",
      "Fold 2:\n",
      "  Train: index=[0 4 6 7 8]\n",
      "  Test:  index=[1 2 3 5 9]\n",
      "Fold 3:\n",
      "  Train: index=[1 2 3 5 9]\n",
      "  Test:  index=[0 4 6 7 8]\n",
      "Fold 4:\n",
      "  Train: index=[0 1 6 7 9]\n",
      "  Test:  index=[2 3 4 5 8]\n",
      "Fold 5:\n",
      "  Train: index=[2 3 4 5 8]\n",
      "  Test:  index=[0 1 6 7 9]\n",
      "Fold 6:\n",
      "  Train: index=[0 3 4 5 8]\n",
      "  Test:  index=[1 2 6 7 9]\n",
      "Fold 7:\n",
      "  Train: index=[1 2 6 7 9]\n",
      "  Test:  index=[0 3 4 5 8]\n",
      "Fold 8:\n",
      "  Train: index=[0 3 6 8 9]\n",
      "  Test:  index=[1 2 4 5 7]\n",
      "Fold 9:\n",
      "  Train: index=[1 2 4 5 7]\n",
      "  Test:  index=[0 3 6 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Here is a simple example of how this splitting occurs\n",
    "X_dummy = X_train[:10] \n",
    "y_dummy = y_train[:10] \n",
    "\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=0)\n",
    "rkf.get_n_splits(X_dummy, y_dummy)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(rkf.split(X_dummy)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c846bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:54.696371Z",
     "start_time": "2023-09-08T20:19:54.586127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here is how 5x2 repeated CV is performed\n",
    "rkf = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=0)\n",
    "\n",
    "pipe1 = pipe_svc = Pipeline(steps=[\n",
    "            (\"scaler\", CorrectedScaler()),\n",
    "            (\"model\", SVC(C=1.0))\n",
    "        ])\n",
    "pipe2 = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "for train_index, test_index in rkf.split(X_train, y_train):\n",
    "    X_train_, X_test_ = X_train[train_index], X_train[test_index]\n",
    "    y_train_, y_test_ = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # Train both pipelines on the same data and evaluate on the same test set\n",
    "    pipe1.fit(X_train_, y_train_)\n",
    "    scores1.append(pipe1.score(X_test_, y_test_))\n",
    "\n",
    "    pipe2.fit(X_train_, y_train_)\n",
    "    scores2.append(pipe2.score(X_test_, y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e75267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:54.993198Z",
     "start_time": "2023-09-08T20:19:54.959990Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can take the difference from each run because both pipelines tested/trained on the same data each time\n",
    "difference = np.array(scores2) - np.array(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5be277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:19:55.418539Z",
     "start_time": "2023-09-08T20:19:55.381345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07042254, -0.12676056, -0.11267606, -0.07042254, -0.05633803,\n",
       "        0.        , -0.05633803, -0.05633803, -0.05633803,  0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "278b634a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:19:02.260862Z",
     "start_time": "2023-09-09T02:19:02.135346Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can get identical results from cross_val_score like this\n",
    "scores1 = cross_val_score(\n",
    "    estimator=pipe1, \n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=rkf,\n",
    ")\n",
    "\n",
    "scores2 = cross_val_score(\n",
    "    estimator=pipe2, \n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=rkf\n",
    ")\n",
    "\n",
    "difference = np.array(scores2) - np.array(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ef95728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:19:02.915078Z",
     "start_time": "2023-09-09T02:19:02.877624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07042254, -0.12676056, -0.11267606, -0.07042254, -0.05633803,\n",
       "        0.        , -0.05633803, -0.05633803, -0.05633803,  0.        ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "29076ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:19:05.530736Z",
     "start_time": "2023-09-09T02:19:05.418011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07042254, -0.12676056, -0.11267606, -0.07042254, -0.05633803,\n",
       "        0.        , -0.05633803, -0.05633803, -0.05633803,  0.        ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyChemAuth has a built in function to perform this.\n",
    "# Be sure any random states are set in your pipelines and to set the random_state here as well.\n",
    "scores1, scores2 = Compare.repeated_kfold(pipe1, pipe2, X_train, y_train, n_repeats=5, k=2, random_state=0)\n",
    "difference = np.array(scores2) - np.array(scores1)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bf82e",
   "metadata": {},
   "source": [
    "**Importantly,** the t-statistic used by Dietterich for his 5x2 CV t-test is modified from a conventional t-statistic, but we will not use it here anyway.  A example implementation is discussed [here](https://www.kaggle.com/code/ogrellier/parameter-tuning-5-x-2-fold-cv-statistical-test/notebook).\n",
    "\n",
    "In general we can perform CV any number of times to get many test performances to improve our assessment of the relative performance. Assuming $n = r \\times k$ runs have been performed, we can define the difference in test set performances between algorithms A and B for a fold in a certain repeat as $x_j = A_j - B_j$, and $\\hat{s}^2 = \\frac{1}{n-1}\\sum(x_j - \\bar{x})^2$ (unbiased sample variance).  This is used with the Student's t distribution and $n-1$ degrees of freedom.\n",
    "\n",
    "$$\n",
    "t = \\frac{ \\frac{1}{n} \\sum_{j=1}^n x_j}{\\sqrt{ \\hat{s}^2 } /\\sqrt{n} } =  \\frac{ \\frac{1}{n} \\sum_{j=1}^n x_j}{\\sqrt{ \\frac{1}{n} \\hat{s}^2 } }\n",
    "$$\n",
    "\n",
    "Dietterich originally found that the standard t-statistic was elevated and therefore needed some correction so he used a different formula;  [Nadeau & Bengio suggested a different correction](https://link.springer.com/article/10.1023/A:1024068626366) to the t-statistic which is designed to \"overestimate the variance to yield conservative inference.\" This has a similar effect of deflating the t-statistic to account for correlations between the data folds. Here, $\\rho = 1/k$ is the fraction of the data used for training.\n",
    "\n",
    "$$\n",
    "t_{\\rm corrected} =  \\frac{ \\frac{1}{n} \\sum_{j=1}^n x_j}{\\sqrt{ \\left( \\frac{1}{n} + \\frac{\\rho}{1 - \\rho} \\right) \\hat{s}^2 } }\n",
    "$$\n",
    "\n",
    "In 2004, [Bouckaert & Frank](https://link.springer.com/chapter/10.1007/978-3-540-24775-3_3) argued that reproducility is more important than minimizing type I error; this depends strongly on how the data is partitioned since different researchers will probably have different RNG seeds when trying to repeat each others' work. Instead they recommend either (a) 100 runs of random resampling or (b) 10×10 repeated cross-validation with the Nadeau and Bengio correction (discussed above).  Interestingly, (a) is the worst thing you can do according to Dietterich, so the latter is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "caa237a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:19:07.728762Z",
     "start_time": "2023-09-09T02:19:07.692963Z"
    }
   },
   "outputs": [],
   "source": [
    "def significance(alpha, scores_svc, scores_tree, n_repeats):\n",
    "    \"\"\"Check the significance of any differences.\"\"\"\n",
    "    p_value = Compare.corrected_t(\n",
    "        scores_svc, # One with the best average\n",
    "        scores_tree,\n",
    "        n_repeats=n_repeats \n",
    "    )\n",
    "\n",
    "    if p_value < alpha: # Reject H0\n",
    "        print('SVC outperforms the Tree ({} < {})'.format('%.4f'%p_value, '%.2f'%alpha)) \n",
    "    else: # Fail to reject H0\n",
    "        print('Pipelines are about the same ({} >= {})'.format('%.4f'%p_value, '%.2f'%alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71d6151f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:19:08.383297Z",
     "start_time": "2023-09-09T02:19:08.344448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x2 Nested CV accuracy for SVC: 0.968 +/- 0.027\n",
      "5x2 Nested CV accuracy for Tree: 0.907 +/- 0.031\n",
      "Pipelines are about the same (0.0934 >= 0.05)\n"
     ]
    }
   ],
   "source": [
    "# You can perform a 2-sided test to check if algorithms are different; however, it is often the case that we develop a \n",
    "# simpler algorithm that might appear to perform a bit worse than some state-of-the-art one, and we need to check if\n",
    "# its underperformance is actually statistically significant.  For this we can perform 1-sided test to see if the mean\n",
    "# performance of the model with the better average is, in fact, better than the one that is worse.\n",
    "\n",
    "# Here the SVC (scores1) seem to be better than the decision tree (scores2).  Let's do a 1-sided hypothesis test.\n",
    "scores1, scores2 = Compare.repeated_kfold(pipe1, pipe2, X_train, y_train, n_repeats=5, k=2, random_state=0)\n",
    "\n",
    "print('%dx%d Nested CV accuracy for SVC: %.3f +/- %.3f' % (5, 2, np.mean(scores1), np.std(scores1)))\n",
    "print('%dx%d Nested CV accuracy for Tree: %.3f +/- %.3f' % (5, 2, np.mean(scores2), np.std(scores2)))\n",
    "\n",
    "significance(alpha=0.05, scores_svc=scores1, scores_tree=scores2, n_repeats=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74ed3da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:21:06.405293Z",
     "start_time": "2023-09-09T02:21:06.020595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x2 Nested CV accuracy for SVC: 0.978 +/- 0.037\n",
      "5x2 Nested CV accuracy for Tree: 0.930 +/- 0.059\n",
      "SVC outperforms the Tree (0.0103 < 0.05)\n"
     ]
    }
   ],
   "source": [
    "# 10x10 is the recommendation, and clearly yields different results!\n",
    "scores1, scores2 = Compare.repeated_kfold(pipe1, pipe2, X_train, y_train, n_repeats=10, k=10, random_state=0)\n",
    "\n",
    "print('%dx%d Nested CV accuracy for SVC: %.3f +/- %.3f' % (5, 2, np.mean(scores1), np.std(scores1)))\n",
    "print('%dx%d Nested CV accuracy for Tree: %.3f +/- %.3f' % (5, 2, np.mean(scores2), np.std(scores2)))\n",
    "\n",
    "significance(alpha=0.05, scores_svc=scores1, scores_tree=scores2, n_repeats=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad384735",
   "metadata": {},
   "source": [
    "The (repeated) nested k-fold paired t-test\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e00935",
   "metadata": {},
   "source": [
    "In the previous notebook we used nested CV because we needed to get an error estimate that accounts for **both the model and its hyperparameter search.**  Until now we have just been comparing pipelines with fixed hyperparameters; often we need to optimize those as well, so nested CV is necessary.  In principle, we can simply apply the corrected t-statistic to the results from the test folds generated by nested CV. (Image credit to [Python Machine Learning, 3rd Ed.](https://github.com/rasbt/python-machine-learning-book-3rd-edition/) by Raschka and Mirjalili.)\n",
    "\n",
    "<!-- ![title](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch06/images/06_07.png?raw=true){ width=500px } -->\n",
    "\n",
    "<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch06/images/06_07.png?raw=true\" style=\"width:500px;\">\n",
    "\n",
    "In this 5x2 nested CV example, we would have 5 test set estimates (r=1, k=5) and can use the corrected t-test above, and/or repeat this several times so that r > 1.\n",
    "\n",
    "There is sometimes confusion surrounding the terminology since nested CV is sometimes referred to as $m \\times n$ *nested* CV, while repeated CV is just called $m \\times n$ CV.  Both are sometimes referred to as simply $5 \\times 2$ CV, but they are **not the same.**\n",
    "\n",
    "See this stackexchange [discussion](https://stats.stackexchange.com/questions/151710/nested-cross-validation-how-is-it-different-from-model-selection-via-kfold-cv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d51cc86f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:15:13.162038Z",
     "start_time": "2023-09-09T02:15:13.118619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nested CV is equivalent to using the repeated k-fold with only 1 repeat, assuming we pass grid search objects \n",
    "# equipped with CV splitters.\n",
    "\n",
    "def compare(k_inner, k_outer, n_repeats):\n",
    "    scores1, scores2 = Compare.repeated_kfold(\n",
    "        pipe1=GridSearchCV(\n",
    "            estimator=Pipeline(steps=[\n",
    "                (\"scaler\", CorrectedScaler()),\n",
    "                (\"model\", SVC())\n",
    "            ]),\n",
    "            param_grid=[{'model__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}],\n",
    "            scoring='accuracy',\n",
    "            cv=sklearn.model_selection.StratifiedKFold( # Since this is a classification problem we will stratify\n",
    "                n_splits=k_inner, # Inner part of the nested CV is handled by this object\n",
    "                shuffle=True,\n",
    "                random_state=0\n",
    "            ),\n",
    "            refit=True # We should refit on the entire training set once optimal hyperparameters are found\n",
    "        ), \n",
    "        pipe2=GridSearchCV(\n",
    "            estimator=DecisionTreeClassifier(random_state=0),\n",
    "            param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n",
    "            scoring='accuracy',\n",
    "            cv=sklearn.model_selection.StratifiedKFold( # Since this is a classification problem we will stratify\n",
    "                n_splits=k_inner, # Inner part of the nested CV is handled by this object\n",
    "                shuffle=True,\n",
    "                random_state=0\n",
    "            ),\n",
    "            refit=True # We should refit on the entire training set once optimal hyperparameters are found\n",
    "        ), \n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        n_repeats=n_repeats, \n",
    "        k=k_outer, \n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    return scores1, scores2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41997b71",
   "metadata": {},
   "source": [
    "Let's review in detail what is happening above.\n",
    "\n",
    "1. `Compare.repeated_kfold` is doing only 1 repeat of a k-fold split of the data it is given.  This is the outer split (k_outer).\n",
    "2. Each estimator is a `GridSearchCV` object which uses k-fold CV to do the inner loop: (1) find optimal hyperparameters, then (2) refit on the entire training fold it was given.\n",
    "3. Each model is optimized, then tested on a single outer test fold so there are k_outer scores.\n",
    "\n",
    "In fact, by specifying `n_repeats` to be more than one, we can simply perform repeated (nested) k-fold CV where we have an internal nested loop to handle hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3fb7b303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:23:04.395563Z",
     "start_time": "2023-09-09T02:23:03.920195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x2 Nested CV accuracy for SVC: 0.979 +/- 0.041\n",
      "5x2 Nested CV accuracy for Tree: 0.915 +/- 0.043\n",
      "Pipelines are about the same (0.0900 >= 0.05)\n"
     ]
    }
   ],
   "source": [
    "# This is just nested k-fold CV where k = k_outer = 5\n",
    "scores1, scores2 = compare(k_inner=2, k_outer=5, n_repeats=1)\n",
    "\n",
    "print('%dx%d Nested CV accuracy for SVC: %.3f +/- %.3f' % (5, 2, np.mean(scores1), np.std(scores1)))\n",
    "print('%dx%d Nested CV accuracy for Tree: %.3f +/- %.3f' % (5, 2, np.mean(scores2), np.std(scores2)))\n",
    "\n",
    "significance(alpha=0.05, scores_svc=scores1, scores_tree=scores2, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0aa41a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:23:27.002993Z",
     "start_time": "2023-09-09T02:23:20.257029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x2 Nested CV accuracy for SVC: 0.975 +/- 0.026\n",
      "5x2 Nested CV accuracy for Tree: 0.924 +/- 0.046\n",
      "SVC outperforms the Tree (0.0194 < 0.05)\n"
     ]
    }
   ],
   "source": [
    "# This is repeated nested k-fold CV where k = k_outer = 5\n",
    "# Similar results were obtained for 10x10 repeated CV\n",
    "scores1, scores2 = compare(k_inner=2, k_outer=5, n_repeats=20)\n",
    "\n",
    "print('%dx%d Nested CV accuracy for SVC: %.3f +/- %.3f' % (5, 2, np.mean(scores1), np.std(scores1)))\n",
    "print('%dx%d Nested CV accuracy for Tree: %.3f +/- %.3f' % (5, 2, np.mean(scores2), np.std(scores2)))\n",
    "\n",
    "significance(alpha=0.05, scores_svc=scores1, scores_tree=scores2, n_repeats=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bbd44c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T02:27:38.615243Z",
     "start_time": "2023-09-09T02:27:21.527594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x2 Nested CV accuracy for SVC: 0.973 +/- 0.041\n",
      "5x2 Nested CV accuracy for Tree: 0.928 +/- 0.063\n",
      "SVC outperforms the Tree (0.0248 < 0.05)\n"
     ]
    }
   ],
   "source": [
    "# 10x10 repeated nested CV could look like this; note k_inner should be appropriate for good hyperparameter tuning\n",
    "# but neither of these \"10s\" refer to this.\n",
    "scores1, scores2 = compare(k_inner=5, k_outer=10, n_repeats=10)\n",
    "\n",
    "print('%dx%d Nested CV accuracy for SVC: %.3f +/- %.3f' % (5, 10, np.mean(scores1), np.std(scores1)))\n",
    "print('%dx%d Nested CV accuracy for Tree: %.3f +/- %.3f' % (5, 10, np.mean(scores2), np.std(scores2)))\n",
    "\n",
    "significance(alpha=0.05, scores_svc=scores1, scores_tree=scores2, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4dfc8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-09T03:15:48.616Z"
    }
   },
   "outputs": [],
   "source": [
    "# It seems that doing more and more testing suggest that the SVC pipeline outperforms the Tree; let's examine this in \n",
    "# more detail. \n",
    "\n",
    "p = {}\n",
    "for n_repeats in np.arange(1, 10+1, 2):\n",
    "    p[n_repeats] = []\n",
    "    for k_outer in [2, 4, 6, 8, 10, 12, 14, 16]:\n",
    "        scores1, scores2 = compare(k_inner=5, k_outer=k_outer, n_repeats=n_repeats)\n",
    "        \n",
    "        p[n_repeats].append(Compare.corrected_t(\n",
    "            scores1, \n",
    "            scores2,\n",
    "            n_repeats=n_repeats \n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45217497",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-09T03:15:48.853Z"
    }
   },
   "outputs": [],
   "source": [
    "# As long as k_outer is ~ 5 or higher, the number of repeats doesn't really affect the p value.\n",
    "\n",
    "# Only very few repeats with very small k_outer (which lead to a small number of test sets, N_tot = N_r * k_outer)\n",
    "# result in p > 0.05 (i.e., the belief that these pipelines perform similarly).  The p-value converges as\n",
    "# N_tot increases.\n",
    "\n",
    "for n in p.keys():\n",
    "    plt.plot([2, 4, 6, 8, 10, 12, 14, 16], p[n], 'o-', label='N_r = {}'.format(n))\n",
    "plt.xlabel('k_outer')\n",
    "plt.ylabel('P')\n",
    "plt.legend(loc='best')\n",
    "_ = plt.axhline(0.05, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c066675",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-09T03:15:50.379Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_biased_ncv(k, k_train=5):\n",
    "    ncv = BiasedNestedCV(k_inner=k_train, k_outer=k)\n",
    "    scores_svc = ncv.grid_search(\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"scaler\", CorrectedScaler()),\n",
    "            (\"model\", SVC())\n",
    "        ]),\n",
    "        param_grid=[{'model__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}],\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        classification=True,\n",
    "    )\n",
    "    \n",
    "    ncv = BiasedNestedCV(k_inner=k_train, k_outer=k)\n",
    "    scores_tree = ncv.grid_search(\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"tree\", DecisionTreeClassifier(random_state=0)),\n",
    "        ]),\n",
    "        param_grid=[{'tree__max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        classification=True,\n",
    "    )\n",
    "    \n",
    "    return scores_svc, scores_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02629dc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-09T03:15:51.101Z"
    }
   },
   "outputs": [],
   "source": [
    "p_biased = []\n",
    "for k_outer in [2, 4, 6, 8, 10, 12, 14, 16]:\n",
    "    scores_svc, scores_tree = compare_biased_ncv(k_outer)\n",
    "    \n",
    "    p_biased.append(\n",
    "        Compare.corrected_t(\n",
    "            scores1=scores_svc, \n",
    "            scores2=scores_tree,\n",
    "            n_repeats=k_outer\n",
    "        )\n",
    "    )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675a6b1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-09T03:15:52.137Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in p.keys():\n",
    "    plt.plot([2, 4, 6, 8, 10], p[n], 'o-', label='N_r = {}'.format(n))\n",
    "plt.xlabel('k_outer')\n",
    "plt.ylabel('P')\n",
    "plt.legend(loc='best')\n",
    "_ = plt.axhline(0.05, color='red')\n",
    "\n",
    "plt.plot([2, 4, 6, 8, 10], p_biased, label='Biased NCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb944c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we typically do atleast 10 outer folds these all appear similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc1215",
   "metadata": {},
   "source": [
    "Rashomon Sets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rudin papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3691b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2658066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T15:08:42.925280Z",
     "start_time": "2023-09-07T15:08:42.905020Z"
    }
   },
   "source": [
    "Bayesian Comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fa215",
   "metadata": {},
   "source": [
    "See \"Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis\", Alessio Benavoli, Giorgio Corani, Janez Demšar, Marco Zaffalon. Journal of Machine Learning Research, 18 (2017) 1-36.\n",
    "\n",
    "They have an associated [github repo](https://github.com/janezd/baycomp) and [documentation](https://baycomp.readthedocs.io/en/latest/) that discusses the advantages of using Bayesian methods to compare ML pipelines instead of frequentist statistics (hypothesis testing).\n",
    "\n",
    "More discussion [here](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/) on the argument for (and against) using Bayesian statistics instead of frequentist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oyster-provenance",
   "language": "python",
   "name": "oyster-provenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
