{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4110398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:57:21.723251Z",
     "start_time": "2023-09-11T18:57:21.715643Z"
    }
   },
   "source": [
    "Partial Least Squares - Discriminant Analysis (PLS-DA)\n",
    "===\n",
    "\n",
    "Author: Nathan A. Mahynski\n",
    "\n",
    "Date: 2023/09/12\n",
    "\n",
    "Description: Discussion and examples of different PLS-DA approaches.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahynski/pychemauth/blob/main/docs/jupyter/gallery/plsda.ipynb)\n",
    "\n",
    "[PLS](pls.ipynb) can also be applied to classification problems.  The general idea is to perform a PLS(2) decomposition\n",
    "between $X$ and $\\vec{y}$, where now $\\vec{y}$ is one-hot encoded for the different classes.  For binary classification, a simple 0 or 1 is adequate.  The scores that come from the PLS decomposition are then used as an input to a\n",
    "classification model.  So really, PLS-DA is just using PLS to find a good subspace, then performing classication\n",
    "on the transformed coordinates in that space; this classification is the \"discrimination\" which can be done\n",
    "by any number of methods.  The PLS outputs floating point numbers not integers, so a decision needs to be made\n",
    "on how to \"cluster\" and assign points to a given class. Some common methods are:\n",
    "\n",
    "* Closest class centroid - obviously that Euclidean distance in PLS-DA score space (projection) is not necessarily a good representation of class differences so you need to validate this with test set, etc. See this [paper](https://link.springer.com/article/10.1007/s11306-007-0099-6) and more discussion in [Pomerantsev et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030).\n",
    "\n",
    "* Thresholding (some cutoff distance).\n",
    "\n",
    "* Some statistical confidence bounds based on tolerable mis-classfication error rates.\n",
    "\n",
    "* Build logistic regression, or other decision boundary, etc. in score (projected) space - this is better than option (1) which avoids relying on distance being proportional to likelihood of a class.\n",
    "\n",
    "This essentially just uses (supervised) PLS to find a good subspace to project into.  However, we could also use something like LDA instead (discussed in the previous section); for LDA we are bounded by `#dimensions <= min(n_classes - 1, n_features)`, which we are not in PLS.  So if `n_classes` is low, LDA can only find a very low-D subspace; it may be better to find some space between that and `n_features`, which PLS can provide.\n",
    "\n",
    "PLS-DA is widely applied in cheminformatics research since we often have a $p > n$ instance which can be handled\n",
    "automatically with PLS.  This is especially true of -omics fields.   However, nonlinear techniques such as [artificial neural networks can also be used](https://link.springer.com/article/10.1007/s11306-020-1640-0) have been shown to perform as well. A thorough review of the state of the art PLS-DA by Lee et al. is available [here](https://pubs.rsc.org/en/content/articlehtml/2018/an/c8an00599k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9) # Automatically restart the runtime to reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pychemauth\n",
    "except:\n",
    "    raise ImportError(\"pychemauth not installed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350867e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03266b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:59:49.776865Z",
     "start_time": "2023-09-11T18:59:49.771092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%watermark` not found.\n"
     ]
    }
   ],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d53af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T17:20:20.233128Z",
     "start_time": "2023-09-19T17:20:20.226609Z"
    }
   },
   "source": [
    "Starting Out\n",
    "---\n",
    "\n",
    "[\"Multiclass partial least squares discriminant analysis: Taking the right wayâ€”A critical tutorial,\" by\n",
    "Pomerantsev and Rodionova, Journal of Chemometrics, 32 (2018)](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030) suggests 2 approaches to PLS-DA. These are \"hard\" and \"soft\" PLS-DA, which are distinguished by how they determine their discrimination boundaries.  Both begin in the same way.  First, let's consider a naive approach to PLS-DA.\n",
    "\n",
    "1. One-hot encode $Y$ for different classes.\n",
    "\n",
    "> This means that classes form the vertices of $k$-dimensional simplex, where we have $k$ classes.  Following [Indahl et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.1061), you could reduce the dummy matrix to remove a dimension;  if you have 4 classes, this forms a tetrahedron - i.e., represent classes as +1 for k-1 instances and for the last one indicate it as all zeros.\n",
    ">\n",
    "> Convert\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> to\n",
    ">\n",
    ">$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    ">The first matrix is connected by a hyperplane and so strictly the space spanned by these classes is $k$-1 dimensional.  This is made more explicitly clear by the second matrix which clearly has rank $k-1$ and encloses a tetrahedral volume.\n",
    "\n",
    "<img src=\"../../_static/fig1_pomerantsev_2018.png\" style=\"width:500px;\">\n",
    "\n",
    "2. Center $X$ (and possibly scale, too), and mean-center $Y$.\n",
    "\n",
    "3. Perform [PLS2 regression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html) to obtain the predictions, $\\hat{Y}$.\n",
    "\n",
    "4. Compare $\\hat{Y}$ to the OHE vectors and choose the closest one to assign a class.  \n",
    "\n",
    "However, step 4 is tricky because of the fact that $\\hat{Y}$ has rank $k$-1.  if you tried to compute a Euclidean distance ($k$-D space) between one vertex and a given prediction, it would be possible - but this would not really be correct since we really need to the distance in the ($k-1$)-D space.  If you just use Indahl's approach and do PLS2 with the second matrix, it seems you can proceed; however, Pomerantsev and Rodionova proposed that you instead use the normal OHE matrix (the first one above) and then perform PCA on the $\\hat{Y}$ response.  The PCA will naturally only have $k-1$ non-zero eigenvalues, so you can simply use this to project the responses into that (hyper)plane.  You can then compute the distances in that hyperplane to perform classification.  These steps are as follows:\n",
    "\n",
    "1. One-hot encode response vector, $Y_{\\rm train}$.\n",
    "\n",
    "2. Perform PLS2 to get prediction, $\\hat{Y}_{\\rm train}$, after appropriate centering/scaling.\n",
    "\n",
    "3. Perform PCA on $\\hat{Y}_{\\rm train}$ - to do this, you must first center $\\hat{Y}_{\\rm train}$. Project into $k-1$ dimensional space to obtain the PCA loadings matrix $T$.\n",
    "\n",
    "4. Classify points in $T$.\n",
    "\n",
    "In summary, $(X, Y) \\xrightarrow{\\text{PLS2}} \\hat{Y} \\xrightarrow{\\text{PCA}} T$.  This is essentially a composition of multiple subspace projection methods, but ultimately only the PCA scores (T) are used to do the classification.\n",
    "\n",
    "This assumes that $k < n_{\\rm features}$.  Since we are projecting into a $k$-D space and relying on a PCA transformation of that space this is a requirement.  Usually when PLS is being used this is the case.\n",
    "\n",
    "Pomerantsev and Rodionova propose 2 different ways to perform step 4: a hard classification and a soft classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5cc3e",
   "metadata": {},
   "source": [
    "Hard PLS-DA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2441b",
   "metadata": {},
   "source": [
    "**Hard classification assigns an object to 1 and only 1 class.**  It is not generally valid to use this approach for authentication studies since this method will not be applicable in \"open world\" conditions where a classifer may encounter classes different from those trained on. Still, this conventional \"closed world\" classifier can still be powerful. See [Rodionova et al.](https://www.sciencedirect.com/science/article/pii/S0165993615302193) for more details.\n",
    "\n",
    "Recall that [LDA](lda.ipynb) uses multivariate Gaussians to model class probabilities; this essentially assigns a point to the class by finding the smallest Mahalanobis distance to a known class center.  We can use a similar approach here.\n",
    "\n",
    "The pooled covariance matrix is known from PCA:\n",
    "\n",
    "$$\n",
    "cov(T) = T^TT = \\Lambda = {\\rm diag}\\left( \\lambda_1, \\dots, \\lambda_{k-1} \\right)\n",
    "$$\n",
    "\n",
    "Is is a diagonal matrix because PCA found orthogonal components.  The class centers, $c_k$, are computed by column centering them (since this was done in the PCA) then using the PCA eigenvectors to project to this $k-1$ dimensional space.\n",
    "\n",
    "The distance from a projected point, $t_i$, to a class, $k$ is given by its Mahalanobis distance:\n",
    "\n",
    "$$\n",
    "d_{ik} = (t_i - c_k)\\Lambda^{-1}(t_i - c_k)^{T}\n",
    "$$\n",
    "\n",
    "Note that depending on if we represent things as columns or rows the transpose may be on the first or second difference term. Importantly, in the P&R paper the authors do not account for the fact that the LDA rule takes into account class prior probability (i.e., class imbalance).  For balanced classes there is no difference between this distance rule and the LDA one.  It is perfectly fine to use the Mahalanobis distance as classification metric, but it is not the same as LDA strictly speaking.  You could also use other classifications approaches like trees, QDA, etc. - it doesn't matter, just classify $T$ based on some method and be sure to appropriately ross validate and test the method.\n",
    "\n",
    "It may be advantageous NOT to have this depend on class prior probabilities since it will influence the (hyper)area of space allocated to a given class.\n",
    "\n",
    "P&R discuss class imbalance as this point was raised in a previous paper they discuss in their work, and show that class imbalance make little to no difference for the dataset(s) they are looking at.  This is of course subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cf8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd02ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c0b00f",
   "metadata": {},
   "source": [
    "Soft PLS-DA\n",
    "---\n",
    "\n",
    "**Soft PLS-DA may assign a point to 1 or more classes, or to none at all.  This means it can be used for authentication studies.**\n",
    "\n",
    "Rather than use LDA (which assumes all classes have the same covariance matrix), P&R suggest using QDA to instead classify T.  The within-class covariance matrix is\n",
    "\n",
    "$$\n",
    "S_k = \\frac{1}{I_k} \\sum_{i \\in \\omega(k)} (t_i-c_k)^T(t_i-c_k)\n",
    "$$\n",
    "\n",
    "where there are $I_k$ samples from class $k$ and $\\omega(k)$ denotes which data points (in the training set) belong to class $k$.  The new Mahalanobis distance follows as:\n",
    "\n",
    "$$\n",
    "d_{ik} = (t_i - c_k)S_k^{-1}(t_i - c_k)^{T}\n",
    "$$\n",
    "\n",
    "Assuming the distances follow a chi-squared distribution we can establish a critical distance below which we assume point i belongs to class $k$, and beyond which we assume it does not.\n",
    "\n",
    "$$\n",
    "d_{\\rm crit} = \\chi^{-2}(1-\\alpha, k-1)\n",
    "$$\n",
    "\n",
    "where $\\chi^{-2}$ is the quantile of the chi-squared distribution with $k$-1 degrees of freedom and a type I error rate of $\\alpha$. $H_0$ is that a point belongs to class $k$; type I error is the rejection of a true $H_0$ (that it belongs to class $k$) so this represents the rate at which we reject that a point belongs to a class that it truly does belong to.  \n",
    "\n",
    "One can also specify an outlier threshold:\n",
    "\n",
    "$$\n",
    "d_{\\rm out} = \\chi^{-2}((1-\\gamma)^{1/I_k}, k-1)\n",
    "$$\n",
    "\n",
    "We expect the [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-square_distribution) since it represents the sum of squres of random variates; here, each direction is assumed to be a random variable and thus, computing a Mahalanobis distance (in a $k$-1 dimensional space) as done above results in a sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba55d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oyster-provenance",
   "language": "python",
   "name": "oyster-provenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
