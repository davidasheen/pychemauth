{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4110398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:57:21.723251Z",
     "start_time": "2023-09-11T18:57:21.715643Z"
    }
   },
   "source": [
    "Partial Least Squares - Discriminant Analysis (PLS-DA)\n",
    "===\n",
    "\n",
    "Author: Nathan A. Mahynski\n",
    "\n",
    "Date: 2023/09/12\n",
    "\n",
    "Description: Discussion and examples of different PLS-DA approaches.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahynski/pychemauth/blob/main/docs/jupyter/gallery/plsda.ipynb)\n",
    "\n",
    "[PLS](pls.ipynb) can also be applied to classification problems.  The general idea is to perform a PLS(2) decomposition\n",
    "between $X$ and $\\vec{y}$, where now $\\vec{y}$ is one-hot encoded for the different classes.  For binary classification, a simple 0 or 1 is adequate.  The scores that come from the PLS decomposition are then used as an input to a\n",
    "classification model.  So really, PLS-DA is just using PLS to find a good subspace, then performing classication\n",
    "on the transformed coordinates in that space; this classification is the \"discrimination\" which can be done\n",
    "by any number of methods.  The PLS outputs floating point numbers not integers, so a decision needs to be made\n",
    "on how to \"cluster\" and assign points to a given class. Some common methods are:\n",
    "\n",
    "1. Closest class centroid - obviously that Euclidean distance in PLS-DA score space (projection) is not necessarily a good representation of class differences so you need to validate this with test set, etc. See this [paper](https://link.springer.com/article/10.1007/s11306-007-0099-6) and more discussion in [Pomerantsev et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030).\n",
    "\n",
    "2. Thresholding (some cutoff distance).\n",
    "\n",
    "3. Some statistical confidence bounds based on tolerable mis-classfication error rates.\n",
    "\n",
    "4. Build logistic regression, or other decision boundary, etc. in score (projected) space - this is better than option (1) which avoids relying on distance being proportional to likelihood of a class.\n",
    "\n",
    "This essentially just uses (supervised) PLS to find a good subspace to project into.  However, we could also use something like LDA instead (discussed in the previous section); for LDA we are bounded by `#dimensions <= min(n_classes - 1, n_features)`, which we are not in PLS.  So if `n_classes` is low, LDA can only find a very low-D subspace; it may be better to find some space between that and `n_features`, which PLS can provide.\n",
    "\n",
    "PLS-DA is widely applied in cheminformatics research since we often have a $p > n$ instance which can be handled\n",
    "automatically with PLS.  This is especially true of -omics fields.   However, nonlinear techniques such as [artificial neural networks can also be used](https://link.springer.com/article/10.1007/s11306-020-1640-0) have been shown to perform as well. A thorough review of the state of the art PLS-DA by Lee et al. is available [here](https://pubs.rsc.org/en/content/articlehtml/2018/an/c8an00599k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949b2197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T13:07:04.198059Z",
     "start_time": "2023-09-20T13:07:04.192963Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9) # Automatically restart the runtime to reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f60adae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T13:07:04.779785Z",
     "start_time": "2023-09-20T13:07:04.327775Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pychemauth\n",
    "except:\n",
    "    raise ImportError(\"pychemauth not installed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a350867e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:02:00.371526Z",
     "start_time": "2023-09-20T16:01:59.516324Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from pychemauth.classifier.plsda import PLSDA\n",
    "from pychemauth.preprocessing.scaling import CorrectedScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03266b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:59:49.776865Z",
     "start_time": "2023-09-11T18:59:49.771092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%watermark` not found.\n"
     ]
    }
   ],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d53af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T17:20:20.233128Z",
     "start_time": "2023-09-19T17:20:20.226609Z"
    }
   },
   "source": [
    "Starting Out\n",
    "---\n",
    "\n",
    "[\"Multiclass partial least squares discriminant analysis: Taking the right wayâ€”A critical tutorial,\" by\n",
    "Pomerantsev and Rodionova, Journal of Chemometrics, 32 (2018)](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030) suggests 2 approaches to PLS-DA. These are \"hard\" and \"soft\" PLS-DA, which are distinguished by how they determine their discrimination boundaries.  Both begin in the same way.  First, let's consider a naive approach to PLS-DA.\n",
    "\n",
    "1. One-hot encode $Y$ for different classes.\n",
    "\n",
    "> This means that classes form the vertices of $k$-dimensional simplex, where we have $k$ classes.  Following [Indahl et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.1061), you could reduce the dummy matrix to remove a dimension;  if you have 4 classes, this forms a tetrahedron - i.e., represent classes as +1 for k-1 instances and for the last one indicate it as all zeros.\n",
    ">\n",
    "> Convert\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> to\n",
    ">\n",
    ">$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    ">The first matrix is connected by a hyperplane and so strictly the space spanned by these classes is $k$-1 dimensional.  This is made more explicitly clear by the second matrix which clearly has rank $k-1$ and encloses a tetrahedral volume.\n",
    "\n",
    "<img src=\"../../_static/fig1_pomerantsev_2018.png\" style=\"width:500px;\">\n",
    "\n",
    "2. Center $X$ (and possibly scale, too), and mean-center $Y$.\n",
    "\n",
    "3. Perform [PLS2 regression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html) to obtain the predictions, $\\hat{Y}$.\n",
    "\n",
    "4. Compare $\\hat{Y}$ to the OHE vectors and choose the closest one to assign a class.  \n",
    "\n",
    "However, step 4 is tricky because of the fact that $\\hat{Y}$ has rank $k$-1.  if you tried to compute a Euclidean distance ($k$-D space) between one vertex and a given prediction, it would be possible - but this would not really be correct since we really need to the distance in the ($k$-1)-D space. Moreover, if you tried to compute a Mahalanobis distance this would fail because the covariance matrix will be singular. If you just use Indahl's approach and do PLS2 with the second matrix, it seems you can proceed; however, Pomerantsev and Rodionova proposed that you instead use the normal OHE matrix (the first one above) and then perform PCA on the $\\hat{Y}$ response.  The PCA will naturally only have $k-1$ non-zero eigenvalues, so you can simply use this to project the responses into that (hyper)plane.  You can then compute the distances in that hyperplane to perform classification.  These steps are as follows:\n",
    "\n",
    "1. One-hot encode response vector, $Y_{\\rm train}$.\n",
    "\n",
    "2. Perform PLS2 to get prediction, $\\hat{Y}_{\\rm train}$, after appropriate centering/scaling.\n",
    "\n",
    "3. Perform PCA on $\\hat{Y}_{\\rm train}$ - to do this, you must first center $\\hat{Y}_{\\rm train}$. Project into $k-1$ dimensional space to obtain the PCA loadings matrix $T$.\n",
    "\n",
    "4. Classify points in $T$.\n",
    "\n",
    "In summary, $(X, Y) \\xrightarrow{\\text{PLS2}} \\hat{Y} \\xrightarrow{\\text{PCA}} T$.  This is essentially a composition of multiple subspace projection methods, but ultimately only the PCA scores (T) are used to do the classification.\n",
    "\n",
    "This assumes that $k < n_{\\rm features}$.  Since we are projecting into a $k$-D space and relying on a PCA transformation of that space this is a requirement.  Usually when PLS is being used this is the case.\n",
    "\n",
    "Pomerantsev and Rodionova propose 2 different ways to perform step 4: a hard classification and a soft classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff927d0",
   "metadata": {},
   "source": [
    "Hard PLS-DA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f727a4",
   "metadata": {},
   "source": [
    "**Hard classification assigns an object to 1 and only 1 class.**  It is not generally valid to use this approach for authentication studies since this method will not be applicable in \"open world\" conditions where a classifer may encounter classes different from those trained on. Still, this conventional \"closed world\" classifier can still be powerful. See [Rodionova et al.](https://www.sciencedirect.com/science/article/pii/S0165993615302193) for more details.\n",
    "\n",
    "Recall that [LDA](lda.ipynb) uses multivariate Gaussians to model class probabilities; this essentially assigns a point to the class by finding the smallest Mahalanobis distance to a known class center.  We can use a similarly inspired approach here.\n",
    "\n",
    "The pooled covariance matrix is known from PCA:\n",
    "\n",
    "$$\n",
    "cov(T^T) = \\Lambda \\sim T^TT \n",
    "$$\n",
    "\n",
    "It is a diagonal matrix because PCA found orthogonal components $\\left( \\Lambda = {\\rm diag}\\left( \\lambda_1, \\dots, \\lambda_{k-1} \\right) \\right)$.  The class centers, $c_k$, are computed by column centering them (since this was done in the PCA step to projection $\\hat{Y}$) then using the PCA eigenvectors to project to this $k-1$ dimensional space.\n",
    "\n",
    "The distance from a projected point, $t_i$, to a class, $k$ is given by its Mahalanobis distance:\n",
    "\n",
    "$$\n",
    "d_{ik} = (t_i - c_k)\\Lambda^{-1}(t_i - c_k)^{T}\n",
    "$$\n",
    "\n",
    "It is perfectly fine to use the Mahalanobis distance as classification metric, but it is not identical to the [LDA](lda.ipynb) decision function, strictly speaking.  You could also use other classifications approaches like trees, QDA, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fdb50281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:09:05.981537Z",
     "start_time": "2023-09-20T16:09:05.929941Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def hard_plsda(X_test, X_train, y_train, n_components=2):\n",
    "    # 1. Preprocess data (one hot encoding, centering)\n",
    "    ohencoder = OneHotEncoder(sparse_output=False) # Convert integers to OHE\n",
    "    x_pls_scaler = CorrectedScaler(with_mean=True, with_std=True) # Center and (optionally) scale X\n",
    "    y_pls_scaler = CorrectedScaler(with_mean=True, with_std=False) # Center do not scale Y\n",
    "\n",
    "    y_train = y_pls_scaler.fit_transform(ohencoder.fit_transform(y_train.reshape(-1,1)))\n",
    "    X_train = x_pls_scaler.fit_transform(X_train)\n",
    "\n",
    "    X_test = x_pls_scaler.transform(X_test)\n",
    "\n",
    "    # 2. PLS2 - predict y_hat\n",
    "    plsda = PLSRegression(n_components=n_components,\n",
    "                          max_iter=5000,\n",
    "                          tol=1.0e-9,\n",
    "                          scale=False) # Already scaled, centered as needed\n",
    "    _ = plsda.fit(X_train, y_train)\n",
    "    \n",
    "    # Y was centered for PLS fitting, so de-center to return to \"normal\"\n",
    "    y_hat_train = y_pls_scaler.inverse_transform(plsda.predict(X_train))\n",
    "    y_hat_test = y_pls_scaler.inverse_transform(plsda.predict(X_test))\n",
    "\n",
    "    # 3. Perform PCA on y_hat_train - use k-1 dimensions\n",
    "    # Note that sklearn's PCA automatically centers the input so we do not need to do it explicitly\n",
    "    pca = PCA(n_components=len(ohencoder.categories_[0])-1, random_state=0)\n",
    "    T_train = pca.fit_transform(y_hat_train) # The means of y_hat_train are stored\n",
    "    T_test = pca.transform(y_hat_test) # The means are re-used here to center\n",
    "    class_centers = pca.transform(np.eye(len(ohencoder.categories_[0]))) # Centers based on y_hat_train automatically\n",
    "\n",
    "    # As a sanity check, proof that covariance of T is diagonal matrix made of eigenvalues from PCA transform\n",
    "    L = np.cov(T_train.T)\n",
    "    assert(np.allclose(np.cov(T_train.T), np.eye(len(pca.explained_variance_))*pca.explained_variance_))\n",
    "\n",
    "    # 4. Compute Mahalanobis distance (squared) and classify based on the smallest distance (squared)\n",
    "    d2 = []\n",
    "    for t in T_test:\n",
    "        d2.append([\n",
    "            np.matmul(\n",
    "                np.matmul(\n",
    "                    (t - class_centers[i]),\n",
    "                    np.linalg.inv(L)),\n",
    "                (t - class_centers[i]).reshape(-1,1)\n",
    "            )[0] for i in range(len(class_centers))\n",
    "        ])\n",
    "    d2 = np.array(d2)\n",
    "    \n",
    "    prediction = np.array([ohencoder.categories_[0][c] for c in np.argmin(d2, axis=1)] )\n",
    "\n",
    "    return prediction, d2, T_train, T_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1f52125e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:10:13.807510Z",
     "start_time": "2023-09-20T16:10:13.407731Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_blobs(\n",
    "    n_samples=100,\n",
    "    n_features=5,\n",
    "    centers=3,\n",
    "    cluster_std=1,\n",
    "    shuffle=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "X_train, y_train = X[:80], Y[:80]\n",
    "X_test, y_test = X[80:], Y[80:]\n",
    "\n",
    "prediction, squared_distances, T_train, T_test = hard_plsda(X_test, X_train, y_train, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ccecf2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:12:01.447032Z",
     "start_time": "2023-09-20T16:12:01.403138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\tActual\tEqual?\n",
      "------------------------------\n",
      "0 \t\t 0 \t True\n",
      "0 \t\t 0 \t True\n",
      "1 \t\t 1 \t True\n",
      "0 \t\t 0 \t True\n",
      "0 \t\t 0 \t True\n",
      "0 \t\t 0 \t True\n",
      "1 \t\t 1 \t True\n",
      "1 \t\t 1 \t True\n",
      "2 \t\t 2 \t True\n",
      "2 \t\t 2 \t True\n",
      "0 \t\t 0 \t True\n",
      "1 \t\t 1 \t True\n",
      "0 \t\t 0 \t True\n",
      "2 \t\t 2 \t True\n",
      "0 \t\t 0 \t True\n",
      "0 \t\t 0 \t True\n",
      "2 \t\t 2 \t True\n",
      "1 \t\t 1 \t True\n",
      "2 \t\t 2 \t True\n",
      "2 \t\t 2 \t True\n"
     ]
    }
   ],
   "source": [
    "# In this case, the PLS-DA predicts the target classes perfectly on the test set.\n",
    "print('Prediction\\tActual\\tEqual?\\n------------------------------')\n",
    "for pred, act in zip(prediction, y_test):\n",
    "    print(pred, '\\t\\t', act, '\\t', pred==act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d82f54df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:11:11.192351Z",
     "start_time": "2023-09-20T16:11:11.139310Z"
    }
   },
   "outputs": [],
   "source": [
    ".# The same calculation can be accomplished with pychemauth\n",
    "plsda = PLSDA(n_components=2, style='hard', scale_x=True)\n",
    "_ = plsda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2c709f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T16:11:13.578647Z",
     "start_time": "2023-09-20T16:11:13.528677Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.all(plsda.predict(X_test) == prediction)\n",
    "assert np.allclose(plsda.transform(X_train), T_train)\n",
    "assert np.allclose(plsda.transform(X_test), T_test)\n",
    "assert np.allclose(plsda.mahalanobis(X_test), squared_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cf9f4",
   "metadata": {},
   "source": [
    "Soft PLS-DA\n",
    "---\n",
    "\n",
    "Rather than using an LDA-like distance (which assumes all classes have the same covariance matrix), you can instead use a QDA-like distance which uses a separate within-class covariance matrix for each class.  Each within-class covariance matrix is given by:\n",
    "\n",
    "$$\n",
    "S_k = \\frac{1}{I_k - 1} \\sum_{i \\in \\omega(k)} (t_i-c_k)^T(t_i-c_k)\n",
    "$$\n",
    "\n",
    "where there are $I_k$ samples from class $k$ and $\\omega(k)$ denotes which data points (in the training set) belong to class $k$.  The Mahalanobis distance (squared) as:\n",
    "\n",
    "$$\n",
    "d_{ik} = (t_i - c_k)S_k^{-1}(t_i - c_k)^{T}\n",
    "$$\n",
    "\n",
    "Assuming the distances follow a chi-squared distribution we can establish a critical distance below which we assume point i belongs to class $k$, and beyond which we assume it does not.\n",
    "\n",
    "$$\n",
    "d_{\\rm crit} = \\chi^{-2}(1-\\alpha, k-1)\n",
    "$$\n",
    "\n",
    "where $\\chi^{-2}$ is the quantile of the chi-squared distribution with $k$-1 degrees of freedom and a type I error rate of $\\alpha$. $H_0$ is that a point belongs to class $k$; type I error is the rejection of a true $H_0$ (that it belongs to class $k$) so this represents the rate at which we reject that a point belongs to a class that it truly does belong to.  \n",
    "\n",
    "One can also specify an outlier threshold:\n",
    "\n",
    "$$\n",
    "d_{\\rm out} = \\chi^{-2}((1-\\gamma)^{1/I_k}, k-1)\n",
    "$$\n",
    "\n",
    "We expect the [chi-squared distribution](https://en.wikipedia.org/wiki/Chi-square_distribution) since it represents the sum of squres of random variates; here, each direction is assumed to be a random variable and thus, computing a Mahalanobis distance (in a $k$-1 dimensional space) as done above results in a sum of squares.\n",
    "\n",
    "**Thus, Soft PLS-DA may assign a point to 1 or more classes, or to none at all.  This means it can be used for authentication studies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b3187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17507c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce215ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oyster-provenance",
   "language": "python",
   "name": "oyster-provenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
