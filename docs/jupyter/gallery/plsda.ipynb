{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4110398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:57:21.723251Z",
     "start_time": "2023-09-11T18:57:21.715643Z"
    }
   },
   "source": [
    "Partial Least Squares - Discriminant Analysis (PLS-DA)\n",
    "===\n",
    "\n",
    "Author: Nathan A. Mahynski\n",
    "\n",
    "Date: 2023/09/12\n",
    "\n",
    "Description: Discussion and examples of different PLS-DA approaches.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahynski/pychemauth/blob/main/docs/jupyter/gallery/plsda.ipynb)\n",
    "\n",
    "[PLS](pls.ipynb) can also be applied to classification problems.  The general idea is to perform a PLS(2) decomposition\n",
    "between $X$ and $\\vec{y}$, where now $\\vec{y}$ is one-hot encoded for the different classes.  For binary classification, a simple 0 or 1 is adequate.  The scores that come from the PLS decomposition are then used as an input to a\n",
    "classification model.  So really, PLS-DA is just using PLS to find a good subspace, then performing classication\n",
    "on the transformed coordinates in that space; this classification is the \"discrimination\" which can be done\n",
    "by any number of methods.  The PLS outputs floating point numbers not integers, so a decision needs to be made\n",
    "on how to \"cluster\" and assign points to a given class. Some common methods are:\n",
    "\n",
    "* Closest class centroid - obviously that Euclidean distance in PLS-DA score space (projection) is not necessarily a good representation of class differences so you need to validate this with test set, etc. See this [paper](https://link.springer.com/article/10.1007/s11306-007-0099-6) and more discussion in [Pomerantsev et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030).\n",
    "\n",
    "* Thresholding (some cutoff distance).\n",
    "\n",
    "* Some statistical confidence bounds based on tolerable mis-classfication error rates.\n",
    "\n",
    "* Build logistic regression, or other decision boundary, etc. in score (projected) space - this is better than option (1) which avoids relying on distance being proportional to likelihood of a class.\n",
    "\n",
    "This essentially just uses (supervised) PLS to find a good subspace to project into.  However, we could also use something like LDA instead (discussed in the previous section); for LDA we are bounded by `#dimensions <= min(n_classes - 1, n_features)`, which we are not in PLS.  So if `n_classes` is low, LDA can only find a very low-D subspace; it may be better to find some space between that and `n_features`, which PLS can provide.\n",
    "\n",
    "PLS-DA is widely applied in cheminformatics research since we often have a $p > n$ instance which can be handled\n",
    "automatically with PLS.  This is especially true of -omics fields.   However, nonlinear techniques such as [artificial neural networks can also be used](https://link.springer.com/article/10.1007/s11306-020-1640-0) have been shown to perform as well. A thorough review of the state of the art PLS-DA by Lee et al. is available [here](https://pubs.rsc.org/en/content/articlehtml/2018/an/c8an00599k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install git+https://github.com/mahynski/pychemauth@main\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9) # Automatically restart the runtime to reload libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pychemauth\n",
    "except:\n",
    "    raise ImportError(\"pychemauth not installed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350867e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03266b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T18:59:49.776865Z",
     "start_time": "2023-09-11T18:59:49.771092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%watermark` not found.\n"
     ]
    }
   ],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d53af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T17:20:20.233128Z",
     "start_time": "2023-09-19T17:20:20.226609Z"
    }
   },
   "source": [
    "Hard vs. Soft PLS-DA\n",
    "---\n",
    "\n",
    "[\"Multiclass partial least squares discriminant analysis: Taking the right wayâ€”A critical tutorial,\" by\n",
    "Pomerantsev and Rodionova, Journal of Chemometrics, 32 (2018)](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.3030) suggests 2 approaches to PLS-DA. These are \"hard\" and \"soft\" PLS-DA, which are distinguished by how they determine their discrimination boundaries.  Both begin in the same way.\n",
    "\n",
    "1. One-hot encode $Y$ for different classes.\n",
    "\n",
    "> This means that classes form the vertices of $k$-dimensional simplex, where we have $k$ classes.  Following [Indahl et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.1061), you could reduce the dummy matrix to remove a dimension;  if you have 4 classes, this forms a tetrahedron - i.e., represent classes as +1 for k-1 instances and for the last one indicate it as all zeros.\n",
    ">\n",
    "> Convert\n",
    ">\n",
    "> $$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    "> to\n",
    ">\n",
    ">$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    ">The first matrix is connected by a hyperplane and so strictly the space spanned by these classes is $k$-1 dimensional.  This is made more explicitly clear by the second matrix which clearly has rank $k-1$ and encloses a tetrahedral volume.\n",
    "\n",
    "<img src=\"../../_static/fig1_pomerantsev_2018.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665c30f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-19T17:28:48.947637Z",
     "start_time": "2023-09-19T17:28:48.822834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbiased_nested_cv.png\u001b[0m*   \u001b[01;32mdefault.png\u001b[0m*              \u001b[01;35mpls_example_fig1.png\u001b[0m\r\n",
      "\u001b[01;32mboruta_in_a_hurry.pdf\u001b[0m*  \u001b[01;32mimblearn_generation.png\u001b[0m*\r\n",
      "\u001b[01;35mcolab_example.gif\u001b[0m       \u001b[01;32mpipeline.png\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../../_static/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe579b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oyster-provenance",
   "language": "python",
   "name": "oyster-provenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
