{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Overview</a></span></li></ul></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Model-the-Data-with-PLS-DA\" data-toc-modified-id=\"Model-the-Data-with-PLS-DA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model the Data with PLS-DA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-a-Hard-Model\" data-toc-modified-id=\"Training-a-Hard-Model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Training a Hard Model</a></span></li><li><span><a href=\"#Training-a-Soft-Model\" data-toc-modified-id=\"Training-a-Soft-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Training a Soft Model</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Testing</a></span></li></ul></li><li><span><a href=\"#Optimizing-the-Classifier\" data-toc-modified-id=\"Optimizing-the-Classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Optimizing the Classifier</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "import imblearn\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import chemometrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "--------\n",
    "This is a simple example of using variants of PLS-DA to do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -t -m -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load some data from the tests/ for this example\n",
    "df = pd.read_csv('../tests/data/plsda3_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that samples are rows, columns are different features\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x = np.array(df.values[:,3:], dtype=float) # Extract features\n",
    "raw_y = np.array(df['Class'].values, dtype=str) # Take the class as the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data with PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemometrics.classifier.plsda import PLSDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Hard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the data are elemental levels so we will scale the X data\n",
    "plsda = PLSDA(n_components=5, \n",
    "              alpha=0.05, \n",
    "              gamma=0.01, \n",
    "              not_assigned='UNKNOWN', \n",
    "              style=\"hard\", \n",
    "              scale_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plsda.fit(raw_x, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plsda.visualize_2d(styles=['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see what samples are predicted to be using the predict() function.\n",
    "pred = plsda.predict(raw_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsda.score(raw_x, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score() function is just tetsing how many are correctly predicted.  You can do this directly and \n",
    "# easily with the \"hard\" version of PLS-DA.\n",
    "np.sum(np.array(pred).ravel() == raw_y) / raw_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complete figures of merit can be computed.\n",
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = plsda.figures_of_merit(pred, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Each row is what the sample IS, each column is what the PREDICTION is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I # Total fo each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNS, TSPS, TEFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Soft Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the data are elemental levels so we will scale the X data\n",
    "plsda = PLSDA(n_components=5, \n",
    "              alpha=0.05, \n",
    "              gamma=0.01, \n",
    "              not_assigned='UNKNOWN', \n",
    "              style=\"soft\", \n",
    "              scale_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plsda.fit(raw_x, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can visualize both the hard and soft boundaries if you train a soft model.\n",
    "# With a hard model, you only get the hard boundaries by default.\n",
    "_ = plsda.visualize_2d(styles=['hard', 'soft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see what samples are predicted to be using the predict() function.\n",
    "pred = plsda.predict(raw_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples can now be predicted to belong to multiple classes.\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complete figures of merit can be computed.\n",
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = plsda.figures_of_merit(pred, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test on other pure samples that weren't in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../tests/data/plsda3_test.csv')\n",
    "raw_x_t = np.array(df.values[:,3:], dtype=float)\n",
    "raw_y_t = np.array(['THA2']*len(raw_x), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = plsda.predict(raw_x_t)\n",
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = plsda.figures_of_merit(pred, raw_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Most foreign samples were CORRECTLY identified as being unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we took alpha as a meaningful choice of type I error rate, but it could also be adjusted.  Moreover, we arbitrarily selected the number of PCs to use in the PLSDA model.  We can use scikit-learn's pipelines to automatically optimize hyperparameters like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I've use an imblearn pipeline, but you can also use scikit-learn's pipeline if you don't want to \n",
    "# do any class balancing.\n",
    "\n",
    "pipeline = imblearn.pipeline.Pipeline(steps=[\n",
    "    # Insert other preprocessing steps here...\n",
    "    # (\"smote\", ScaledSMOTEENN(random_state=1)), # For example, class balancing\n",
    "    (\"plsda\", PLSDA(n_components=5, \n",
    "                    alpha=0.05,\n",
    "                    scale_x=True, \n",
    "                    not_assigned='UNKNOWN',\n",
    "                    style='soft', \n",
    "                   )\n",
    "    )\n",
    "])\n",
    "\n",
    "# Hyperparameters of pipeline steps are given in standard notation: step__parameter_name\n",
    "param_grid = [{\n",
    "    # 'smote__k_enn':[1, 2, 3],\n",
    "    # 'smote__k_smote':[1, 3, 3],\n",
    "    # 'smote__kind_sel_enn':['all', 'mode'],\n",
    "    'plsda__n_components':np.arange(1, 20, 2),\n",
    "    'plsda__alpha': [0.07, 0.05, 0.03, 0.01],\n",
    "    #'plsda__scale_x':[True, False],\n",
    "    #'plsda__style':['hard', 'soft'],\n",
    "}]\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=-1,\n",
    "    cv=sklearn.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=0),\n",
    "    error_score=0,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "_ = gs.fit(raw_x, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters found can be accessed like this:\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_ # The best score it recieved was..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see detailed CV results here\n",
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a 1D optimization you can easily visualize where the best value is:\n",
    "# plt.errorbar(np.arange(1, 20, 2), gs.cv_results_['mean_test_score'], yerr=gs.cv_results_['std_test_score'])\n",
    "# plt.xlabel('n_components')\n",
    "# plt.ylabel('Mean Test Score (TEFF)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn finds the optimum over the range, however, you may wish to simply look at these results\n",
    "# and use a smaller value, perhaps at an \"elbow\", and re-train a new model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The refit=True (default) refits the model on the data in the end so you can use it directly.\n",
    "gs.best_estimator_.predict(raw_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can visualize the training results\n",
    "gs.best_estimator_.named_steps['plsda'].visualize_2d(styles=['hard', 'soft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "gs.best_estimator_.named_steps['plsda'].score(raw_x, raw_y) # The score being used here is TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gs.best_estimator_.named_steps['plsda'].predict(raw_x)\n",
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = plsda.figures_of_merit(pred, raw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSPS, TSNS, TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(gs.best_estimator_.named_steps['plsda'].check_outliers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "gs.best_estimator_.named_steps['plsda'].score(raw_x_t, raw_y_t) # The score being used here is TEFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gs.best_estimator_.named_steps['plsda'].predict(raw_x_t)\n",
    "df, I, CSNS, CSPS, CEFF, TSNS, TSPS, TEFF = plsda.figures_of_merit(pred, raw_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
